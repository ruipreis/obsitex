@article{acharBackPainChildren2020,
 abstract = {Back pain is a relatively common presenting symptom in children and adolescents. Typical causes include muscle strain or spasm, spinal deformities (e.g., Scheuermann kyphosis, adolescent idiopathic scoliosis), spondylolysis, bulging or herniated intervertebral disks, apophysitis of the iliac crest, and functional pain syndromes such as fibromyalgia. Spondyloarthropathies such as ankylosing spondylitis may present with low back pain and stiffness, which are often worse in the morning. Less common but more serious causes include malignancy and infections. The physical examination should include postural inspection, evaluation for tenderness, range of motion, strength testing, and testing for fractures and nerve impingement. Treatment for patients with muscle strain include relative rest, home-based exercises, physical therapy, and limited use of nonsteroidal anti-inflammatory drugs. If findings from the history and physical examination suggest underlying pathology, radiography and laboratory studies are indicated initially; magnetic resonance imaging, computed tomography, or a bone scan may be needed for further evaluation. It is generally accepted that the following factors warrant immediate evaluation: patient age younger than five years, symptoms persisting beyond four weeks, systemic symptoms, nighttime pain, bowel incontinence/urinary retention, or other neurologic symptoms.},
 author = {Achar, Suraj and Yamanaka, Jarrod},
 issn = {1532-0650},
 journal = {Am Fam Physician},
 keywords = {Adolescent,Anti-Inflammatory Agents Non-Steroidal,Back Pain,Child,Diagnosis Differential,Exercise Therapy,Female,Humans,Magnetic Resonance Imaging,Male,Medical History Taking,Neurologic Examination,Pediatrics,Physical Examination,Risk Assessment,Symptom Assessment,Tomography X-Ray Computed},
 langid = {english},
 month = {July},
 number = {1},
 pages = {19--28},
 pmid = {32603067},
 title = {Back {{Pain}} in {{Children}} and {{Adolescents}}},
 volume = {102},
 year = {2020}
}

@misc{aghajanyanIntrinsicDimensionalityExplains2020,
 abstract = {Although pretrained language models can be fine-tuned to produce state-of-the-art results for a very wide range of language understanding tasks, the dynamics of this process are not well understood, especially in the low data regime. Why can we use relatively vanilla gradient descent algorithms (e.g., without strong regularization) to tune a model with hundreds of millions of parameters on datasets with only hundreds or thousands of labeled examples? In this paper, we argue that analyzing fine-tuning through the lens of intrinsic dimension provides us with empirical and theoretical intuitions to explain this remarkable phenomenon. We empirically show that common pre-trained models have a very low intrinsic dimension; in other words, there exists a low dimension reparameterization that is as effective for fine-tuning as the full parameter space. For example, by optimizing only 200 trainable parameters randomly projected back into the full space, we can tune a RoBERTa model to achieve 90{\textbackslash}\% of the full parameter performance levels on MRPC. Furthermore, we empirically show that pre-training implicitly minimizes intrinsic dimension and, perhaps surprisingly, larger models tend to have lower intrinsic dimension after a fixed number of pre-training updates, at least in part explaining their extreme effectiveness. Lastly, we connect intrinsic dimensionality with low dimensional task representations and compression based generalization bounds to provide intrinsic-dimension-based generalization bounds that are independent of the full parameter count.},
 archiveprefix = {arXiv},
 author = {Aghajanyan, Armen and Zettlemoyer, Luke and Gupta, Sonal},
 doi = {10.48550/arXiv.2012.13255},
 eprint = {2012.13255},
 month = {December},
 number = {arXiv:2012.13255},
 primaryclass = {cs},
 publisher = {arXiv},
 title = {Intrinsic {{Dimensionality Explains}} the {{Effectiveness}} of {{Language Model Fine-Tuning}}},
 urldate = {2023-08-11},
 year = {2020}
}

@misc{weiFinetunedLanguageModels2022,
 abstract = {This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning -- finetuning language models on a collection of tasks described via instructions -- substantially improves zero-shot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction-tune it on over 60 NLP tasks verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 tasks that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of finetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning.},
 archiveprefix = {arXiv},
 author = {Wei, Jason and Bosma, Maarten and Zhao, Vincent Y. and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M. and Le, Quoc V.},
 eprint = {2109.01652},
 keywords = {Computer Science - Computation and Language},
 month = {February},
 number = {arXiv:2109.01652},
 primaryclass = {cs},
 publisher = {arXiv},
 title = {Finetuned {{Language Models Are Zero-Shot Learners}}},
 urldate = {2023-10-25},
 year = {2022}
}
